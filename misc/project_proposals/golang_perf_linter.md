# Goalng Perf Linter -- Because Naming Things Is Hard

NOTE: see summary below. Perf linting is strongly supported by existing tools like pprof.

A common problem in development is that developers grind lots of time in the
starting phases of a project: requirements, specification, evaluating technoogy/libraries, and then prototype coding. The way I describe it is like trying to summit a mountain but spending too much
time in the lowlands and valleys.

One effect of this is that it takes a very long time (too long) to get feedback per how an
application will run, behave, and perform at runtime in a real system. This means that memory
leaks almost inevitably find their way into a code base, since the dynamical behavior of an
app/system is not observed until the very end of development, or worst yet, in production.
For developers the experience is pure heartburn.

The proposed Golang Perf Linter is a tool for integrating runtime/dynamical analysis of 
a golang app into the development process, pushing this feedback further upstream in the development
lifecycle. Golang provides tremendous tools for analyzing the runtime (memory usage, cpu usage,
goroutines, even flamecharts), so the goal is simply to shift these datapoints further upstream
in the development process, into the awareness of the developer. The expectation is that this
continuous feedback will catch dynamical issues earlier in the process:
* memory leaks
* compute/cloud costs due to:
    * underutilization: not using all of one's allocated cloud resource/budget
    * overutilization: reducing resource consumption to reduce costs

### Shotgun Proposals
1) Sidecar perfmon: in the most ideal scenario, a perf monitor could be injected into
pods as a sidecar, separating its maintenance and deployment entirely. This is absurd, obviously,
since pprof must be compiled into an application. But it elucidates the ultimate goal of
performance monitoring, and in fact this could be done to monitor overall utilization datapoints
using berkeley packet filters, the /proc directory, cgroup info, and so forth.
2) A golang build flag: using a simple build flag one could instantiate the perfmon and start in the init function of a separate code file. This file would be compiled only if the build flag was passed.
This is the best method for implementing this tool in golang, since pprof must be compiled into
the app binary.
3) A full build environment/container whose output is a profile: for example, a go-compiler container
monitoring for file changes, compiling, and possibly even injecting the pprof code. The issue here
would be making such an environment resource-aware, for example a developer ideally has an environment
like tilt for which their app is deployed immediately into an environment with mock or real resources (services, dbs, encryption, etc).
4) Integrate pprof with testing: profiling dovetails with testing, and is not such a bad way to go.
For developers, continuous feedback could really just be an annoyance until needed. 

Conclusions:
* The primary approaches seem to be:
    1) a realtime monitor built-in using a build-flag
    2) a test-based profiler that runs in the background of an integration test.

### Use-cases
1) One shot evaluation: run the app temporarily and export some prof artifacts
2) Continuous evaluation: gather metrics and serve them over http

### Ups and Downs
+ Golang's pprof tooling provides all requirements from the get-go
- pprof must be compiled into the app binary and runtime
- pprof may not be 100% cross-platform, but this is a problem for the go team

## Summary

Pprof is batteries-included, there are few use-cases described above that are not supported by
its existing tooling.
1) Run `go test` with profile flags: cpupofile, memprofile, blockprofile
2) Add the pprof server to one's code and compile it using build flags for realtime metric snapshots
3) Develop one's client in parallel with server development and test with it. This is the more complex
   option, as the client could implement more complex load testing, but would look something like this for a gRPC endpoint backed by postgres:
    1) Make some code changes to the server
    2) Run postgres and other external dependencies
    3) Run the gRPC/postgres endpoints 
    4) Use one's client (generated by protoc, extended by your code) to automate large volumes of queries, etc
    5) Monitor metrics using the pprof http endpoints to look for problems like monotonic increases in memory

There are many alternatives, and all can be modified to a specific use-case.
For instance (3) could be implemented fully within a benchmark integration-test; this
would immediately flag any code modifications that adversely impacted performance in any way.

But in summary:
* Use pprof tooling natively, no additional metric-app is needed
* Use platform metrics (prometheus, cgroup and /proc info, etc) for general issue-spotting

KISS.